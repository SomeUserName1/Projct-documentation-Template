% Einstein Robot quotation
@ARTICLE{7534850,
author={A. Zaraki and M. Pieroni and D. De Rossi and D. Mazzei and R. Garofalo and L. Cominelli and M. B. Dehkordi},
journal={IEEE Transactions on Cognitive and Developmental Systems},
title={Design and Evaluation of a Unique Social Perception System for Human–Robot Interaction},
year={2017},
volume={9},
number={4},
pages={341-355},
keywords={humanoid robots;human-robot interaction;social aspects of automation;software architecture;humanoid robots;unique social perception system;high-level tasks;high-level features;facial expressions;body gestures;robotics software architectures;hardware platforms;highly customized solutions;SPS detects;system capability;high-level perceptual features;human-robot interaction;robotic platforms;social robotics platforms;general-robot interaction;HRI contexts;Feature extraction;Robot sensing systems;Face recognition;Context awareness;Cameras;Human-robot interaction;Image analysis;Context-aware social perception;humanoid social robots;human–robot interaction (HRI);meta-scene;platform-independent system;scene analysis},
doi={10.1109/TCDS.2016.2598423},
ISSN={2379-8920},
month={Dec},}

% MTL Survey
@article{DBLP:journals/corr/Ruder17a,
  author    = {Sebastian Ruder},
  title     = {An Overview of Multi-Task Learning in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.05098},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.05098},
  archivePrefix = {arXiv},
  eprint    = {1706.05098},
  timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Ruder17a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% MTL Survey
@article{DBLP:journals/corr/ZhangY17aa,
  author    = {Yu Zhang and
               Qiang Yang},
  title     = {A Survey on Multi-Task Learning},
  journal   = {CoRR},
  volume    = {abs/1707.08114},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.08114},
  archivePrefix = {arXiv},
  eprint    = {1707.08114},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhangY17aa},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% transfer learning
@book{pratt1993transferring,
  title={Transferring previously learned back-propagation neural networks to new learning tasks},
  author={Pratt, Lorien Y},
  year={1993},
  publisher={Citeseer}
}

%Knowledge tranfer social networks
@article{argote2000knowledge,
  title={Knowledge transfer: A basis for competitive advantage in firms},
  author={Argote, Linda and Ingram, Paul},
  journal={Organizational behavior and human decision processes},
  volume={82},
  number={1},
  pages={150--169},
  year={2000},
  publisher={Elsevier}
}

% task factorization humans
@article {FleschE10313,
	author = {Flesch, Timo and Balaguer, Jan and Dekker, Ronald and Nili, Hamed and Summerfield, Christopher},
	title = {Comparing continual task learning in minds and machines},
	volume = {115},
	number = {44},
	pages = {E10313--E10322},
	year = {2018},
	doi = {10.1073/pnas.1800755115},
	publisher = {National Academy of Sciences},
	abstract = {Humans learn to perform many different tasks over the lifespan, such as speaking both French and Spanish. The brain has to represent task information without mutual interference. In machine learning, this {\textquotedblleft}continual learning{\textquotedblright} is a major unsolved challenge. Here, we studied the patterns of errors made by humans and state-of-the-art neural networks while they learned new tasks from scratch and without instruction. Humans, but not machines, seem to benefit from training regimes that blocked one task at a time, especially when they had a prior bias to represent stimuli in a way that encouraged task separation. Machines trained to exhibit the same prior bias suffered less interference between tasks, suggesting new avenues for solving continual learning in artificial systems.Humans can learn to perform multiple tasks in succession over the lifespan ({\textquotedblleft}continual{\textquotedblright} learning), whereas current machine learning systems fail. Here, we investigated the cognitive mechanisms that permit successful continual learning in humans and harnessed our behavioral findings for neural network design. Humans categorized naturalistic images of trees according to one of two orthogonal task rules that were learned by trial and error. Training regimes that focused on individual rules for prolonged periods (blocked training) improved human performance on a later test involving randomly interleaved rules, compared with control regimes that trained in an interleaved fashion. Analysis of human error patterns suggested that blocked training encouraged humans to form {\textquotedblleft}factorized{\textquotedblright} representation that optimally segregated the tasks, especially for those individuals with a strong prior bias to represent the stimulus space in a well-structured way. By contrast, standard supervised deep neural networks trained on the same tasks suffered catastrophic forgetting under blocked training, due to representational interference in the deeper layers. However, augmenting deep networks with an unsupervised generative model that allowed it to first learn a good embedding of the stimulus space (similar to that observed in humans) reduced catastrophic forgetting under blocked training. Building artificial agents that first learn a model of the world may be one promising route to solving continual task performance in artificial intelligence research.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/115/44/E10313},
	eprint = {http://www.pnas.org/content/115/44/E10313.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

% Relational biases, deep learning and graph networks
@misc{battaglia2018relational,
    title={Relational inductive biases, deep learning, and graph networks},
    author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
    year={2018},
    eprint={1806.01261},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Caruana1993MultitaskLA,
  title={Multitask Learning: A Knowledge-Based Source of Inductive Bias},
  author={Rich Caruana},
  booktitle={ICML},
  year={1993}
}

% Multi-Task origin
@Article{Caruana1997,
author="Caruana, Rich",
title="Multitask Learning",
journal="Machine Learning",
year="1997",
month="Jul",
day="01",
volume="28",
number="1",
pages="41--75",
abstract="Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.",
issn="1573-0565",
doi="10.1023/A:1007379606734",
url="https://doi.org/10.1023/A:1007379606734"
}

%relational inductive bias mutli task
@article{DBLP:journals/corr/abs-1106-0245,
  author    = {Jonathan Baxter},
  title     = {A Model of Inductive Bias Learning},
  journal   = {CoRR},
  volume    = {abs/1106.0245},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.0245},
  archivePrefix = {arXiv},
  eprint    = {1106.0245},
  timestamp = {Mon, 13 Aug 2018 16:47:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1106-0245},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Baxter1997,
author="Baxter, Jonathan",
title="A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling",
journal="Machine Learning",
year="1997",
month="Jul",
day="01",
volume="28",
number="1",
pages="7--39",
abstract="A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks.",
issn="1573-0565",
doi="10.1023/A:1007327622663",
url="https://doi.org/10.1023/A:1007327622663"
}

multi-task feature learning
@inproceedings{Argyriou2006MultiTaskFL,
  title={Multi-Task Feature Learning},
  author={Andreas Argyriou and Theodoros Evgeniou and Massimiliano Pontil},
  booktitle={NIPS},
  year={2006}
}

% l1-norm sparsest
@article{donoho2006most,
  title={For most large underdetermined systems of linear equations the minimal $l_1$-norm solution is also the sparsest solution},
  author={Donoho, David L},
  journal={Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
  volume={59},
  number={6},
  pages={797--829},
  year={2006},
  publisher={Wiley Online Library}
}

% l1-norm
@article{doi:10.1137/0907087,
author = {Santosa, F. and Symes, W.},
title = {Linear Inversion of Band-Limited Reflection Seismograms},
journal = {SIAM Journal on Scientific and Statistical Computing},
volume = {7},
number = {4},
pages = {1307-1330},
year = {1986},
doi = {10.1137/0907087},
URL = {  https://doi.org/10.1137/0907087},
eprint = { https://doi.org/10.1137/0907087}
}

% group lasso
@ARTICLE{Yuan06modelselection,
    author = {Ming Yuan and Yi Lin},
    title = {Model selection and estimation in regression with grouped variables},
    journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
    year = {2006},
    volume = {68},
    pages = {49--67}
}

% inf. gropup lasso
@article{zhang2008,
author = "Zhang, Cun-Hui and Huang, Jian",
doi = "10.1214/07-AOS520",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "08",
number = "4",
pages = "1567--1594",
publisher = "The Institute of Mathematical Statistics",
title = "The sparsity and bias of the Lasso selection in high-dimensional linear regression",
url = "https://doi.org/10.1214/07-AOS520",
volume = "36",
year = "2008"
}

@article{meinshausen2006,
author = "Meinshausen, Nicolai and Bühlmann, Peter",
doi = "10.1214/009053606000000281",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "3",
pages = "1436--1462",
publisher = "The Institute of Mathematical Statistics",
title = "High-dimensional graphs and variable selection with the Lasso",
url = "https://doi.org/10.1214/009053606000000281",
volume = "34",
year = "2006"
}

@misc{bakin1999adaptive,
  title={Adaptive regression and model selection in data mining problems},
  author={Bakin, Sergey and others},
  year={1999},
  publisher={The Australian National University}
}

@article{evgeniou2005learning,
  title={Learning multiple tasks with kernel methods},
  author={Evgeniou, Theodoros and Micchelli, Charles A and Pontil, Massimiliano},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={615--637},
  year={2005}
}

@inproceedings{thrun1996discovering,
  title={Discovering structure in multiple learning tasks: The TC algorithm},
  author={Thrun, Sebastian and O'Sullivan, Joseph},
  booktitle={ICML},
  volume={96},
  pages={489--497},
  year={1996}
}

@misc{heskes2000empirical,
  title={Empirical Bayes for learning to learn},
  author={Heskes, TM},
  year={2000},
  publisher={San Francisco: Morgan Kaufmann}
}

@article{DBLP:journals/corr/Long015a,
  author    = {Mingsheng Long and
               Jianmin Wang},
  title     = {Learning Multiple Tasks with Deep Relationship Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02117},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02117},
  archivePrefix = {arXiv},
  eprint    = {1506.02117},
  timestamp = {Mon, 13 Aug 2018 16:49:17 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Long015a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LuKZCJF16,
  author    = {Yongxi Lu and
               Abhishek Kumar and
               Shuangfei Zhai and
               Yu Cheng and
               Tara Javidi and
               Rog{\'{e}}rio Schmidt Feris},
  title     = {Fully-adaptive Feature Sharing in Multi-Task Networks with Applications
               in Person Attribute Classification},
  journal   = {CoRR},
  volume    = {abs/1611.05377},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05377},
  archivePrefix = {arXiv},
  eprint    = {1611.05377},
  timestamp = {Mon, 13 Aug 2018 16:47:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LuKZCJF16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MisraSGH16,
  author    = {Ishan Misra and
               Abhinav Shrivastava and
               Abhinav Gupta and
               Martial Hebert},
  title     = {Cross-stitch Networks for Multi-task Learning},
  journal   = {CoRR},
  volume    = {abs/1604.03539},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.03539},
  archivePrefix = {arXiv},
  eprint    = {1604.03539},
  timestamp = {Mon, 13 Aug 2018 16:46:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MisraSGH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-08328,
  author    = {Amir Roshan Zamir and
               Alexander Sax and
               William B. Shen and
               Leonidas J. Guibas and
               Jitendra Malik and
               Silvio Savarese},
  title     = {Taskonomy: Disentangling Task Transfer Learning},
  journal   = {CoRR},
  volume    = {abs/1804.08328},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.08328},
  archivePrefix = {arXiv},
  eprint    = {1804.08328},
  timestamp = {Mon, 13 Aug 2018 16:47:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-08328},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/KendallGC17,
  author    = {Alex Kendall and
               Yarin Gal and
               Roberto Cipolla},
  title     = {Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry
               and Semantics},
  journal   = {CoRR},
  volume    = {abs/1705.07115},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07115},
  archivePrefix = {arXiv},
  eprint    = {1705.07115},
  timestamp = {Mon, 13 Aug 2018 16:48:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KendallGC17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}






THEORY
stdlib
https://www.deeplearningbook.org/


LOSS FUNCTION ADAPTION and non diff optimization:
Conditional gradients
https://arxiv.org/pdf/1810.02429.pdf
Uncertainty based loss
https://arxiv.org/pdf/1705.07115.pdf

@inproceedings{Chen2018GradNormGN,
  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},
  author={Zhao Chen and Vijay Badrinarayanan and Chen-Yu Lee and Andrew Rabinovich},
  booktitle={ICML},
  year={2018}
}

FRANK-WOLFE ALGO
https://arxiv.org/pdf/1811.07591.pdf
https://arxiv.org/pdf/1807.07680.pdf (MIT)
http://proceedings.mlr.press/v28/jaggi13.pdf
https://link.springer.com/content/pdf/10.1007%2F978-3-030-01421-6.pdf
Simple expl.
http://www.math.chalmers.se/Math/Grundutb/CTH/tma946/0203/fw_eng.pdf

LASSO
convex multi task feature
https://link.springer.com/content/pdf/10.1007/s10994-007-5040-8.pdf
Group lasso
http://pages.stat.wisc.edu/~myuan/papers/glasso.final.pdf
bad group lasso 
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.7164&rep=rep1&type=pdf
mixed lassos
https://papers.nips.cc/paper/4125-a-dirty-model-for-multi-task-learning.pdf
distr. mixed lasso
https://arxiv.org/pdf/1612.04022.pdf

COMPUTABILITY
Automata Studies Shannon
https://dl.acm.org/citation.cfm?id=1096884
Task priorization
https://www.semanticscholar.org/paper/Dynamic-Task-Prioritization-for-Multitask-Learning-Guo-Haque/f55c1b8b276620c4071641d7f1bd3468ed2d4188

RELATION INFERENCE
Schema, see bottom, RCN, see bottom
Relation inference networks
https://arxiv.org/pdf/1802.04687.pdf
Hints
https://www.sciencedirect.com/science/article/pii/0885064X9090006Y
Task-Coupling
https://people.eecs.berkeley.edu/~russell/classes/cs294/f05/papers/evgeniou+pontil-2004.pdf

LIFELONG
Lifelong learning (Transfer)
https://arxiv.org/abs/1311.2838

TOREAD
Multi-Modal multi-task learning
https://www.sciencedirect.com/science/article/pii/S105381191101144X
multi modal latent attributes
https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/6368/FuEtAl_PAMI2014_GREEN.pdf?sequence=2

MODELS
Sluice 
https://arxiv.org/pdf/1705.08142.pdf
understanding beta vae
https://arxiv.org/pdf/1804.03599.pdf
beta-vae
https://openreview.net/pdf?id=Sy2fzU9gl
semi-supervised generative
https://arxiv.org/abs/1406.5298

EXAMPLES 
Audio-Visual object detection
https://link.springer.com/chapter/10.1007/978-3-030-01267-0_34
Robust visual tracking using MTL
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.389.5393&rep=rep1&type=pdf
multi-modal deep learning
http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf
Multi label image classification cnn-rnn
https://arxiv.org/ftp/arxiv/papers/1604/1604.04573.pdf
Deep multi-modal feature analysis
https://arxiv.org/pdf/1603.07120.pdf
multi modal multipart action recognition
https://arxiv.org/abs/1507.08761

Human activity modeling 
https://arxiv.org/pdf/1809.08875.pdf
Faces and face based
https://www.researchgate.net/profile/Chen_Change_Loy/publication/264786906_Facial_Landmark_Detection_by_Deep_Multi-task_Learning/links/53f05b800cf2711e0c42ff77/Facial-Landmark-Detection-by-Deep-Multi-task-Learning.pdf


% natural learning/learning by answering
@article{DBLP:journals/corr/abs-1712-01238,
  author    = {Ishan Misra and
               Ross B. Girshick and
               Rob Fergus and
               Martial Hebert and
               Abhinav Gupta and
               Laurens van der Maaten},
  title     = {Learning by Asking Questions},
  journal   = {CoRR},
  volume    = {abs/1712.01238},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01238},
  archivePrefix = {arXiv},
  eprint    = {1712.01238},
  timestamp = {Mon, 13 Aug 2018 16:47:15 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01238},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



% Schema networks: Zero shot transfer RL
@article{DBLP:journals/corr/KanskySMELLDSPG17,
  author    = {Ken Kansky and
               Tom Silver and
               David A. M{\'{e}}ly and
               Mohamed Eldawy and
               Miguel L{\'{a}}zaro{-}Gredilla and
               Xinghua Lou and
               Nimrod Dorfman and
               Szymon Sidor and
               D. Scott Phoenix and
               Dileep George},
  title     = {Schema Networks: Zero-shot Transfer with a Generative Causal Model
               of Intuitive Physics},
  journal   = {CoRR},
  volume    = {abs/1706.04317},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04317},
  archivePrefix = {arXiv},
  eprint    = {1706.04317},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KanskySMELLDSPG17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%Recurent Cortical Networks
@article {Georgeeaag2612,
	author = {George, Dileep and Lehrach, Wolfgang and Kansky, Ken and L{\'a}zaro-Gredilla, Miguel and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zhaoshi and Liu, Yi and Wang, Huayan and Lavin, Alex and Phoenix, D. Scott},
	title = {A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs},
	volume = {358},
	number = {6368},
	year = {2017},
	doi = {10.1126/science.aag2612},
	publisher = {American Association for the Advancement of Science},
	abstract = {Proving that we are human is now part of many tasks that we do on the internet, such as creating an email account, voting in an online poll, or even downloading a scientific paper. One of the most popular tests is text-based CAPTCHA, where would-be users are asked to decipher letters that may be distorted, partially obscured, or shown against a busy background. This test is used because computers find it tricky, but (most) humans do not. George et al. developed a hierarchical model for computer vision that was able to solve CAPTCHAs with a high accuracy rate using comparatively little training data. The results suggest that moving away from text-based CAPTCHAs, as some online services have done, may be a good idea.Science, this issue p. eaag2612INTRODUCTIONCompositionality, generalization, and learning from a few examples are among the hallmarks of human intelligence. CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart), images used by websites to block automated interactions, are examples of problems that are easy for people but difficult for computers. CAPTCHAs add clutter and crowd letters together to create a chicken-and-egg problem for algorithmic classifiers{\textemdash}the classifiers work well for characters that have been segmented out, but segmenting requires an understanding of the characters, which may be rendered in a combinatorial number of ways. CAPTCHAs also demonstrate human data efficiency: A recent deep-learning approach for parsing one specific CAPTCHA style required millions of labeled examples, whereas humans solve new styles without explicit training.By drawing inspiration from systems neuroscience, we introduce recursive cortical network (RCN), a probabilistic generative model for vision in which message-passing{\textendash}based inference handles recognition, segmentation, and reasoning in a unified manner. RCN learns with very little training data and fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters. In addition, RCN outperforms deep neural networks on a variety of benchmarks while being orders of magnitude more data-efficient.RATIONALEModern deep neural networks resemble the feed-forward hierarchy of simple and complex cells in the neocortex. Neuroscience has postulated computational roles for lateral and feedback connections, segregated contour and surface representations, and border-ownership coding observed in the visual cortex, yet these features are not commonly used by deep neural nets. We hypothesized that systematically incorporating these findings into a new model could lead to higher data efficiency and generalization. Structured probabilistic models provide a natural framework for incorporating prior knowledge, and belief propagation (BP) is an inference algorithm that can match the cortical computational speed. The representational choices in RCN were determined by investigating the computational underpinnings of neuroscience data under the constraint that accurate inference should be possible using BP.RESULTSRCN was effective in breaking a wide variety of CAPTCHAs with very little training data and without using CAPTCHA-specific heuristics. By comparison, a convolutional neural network required a 50,000-fold larger training set and was less robust to perturbations to the input. Similar results are shown on one- and few-shot MNIST (modified National Institute of Standards and Technology handwritten digit data set) classification, where RCN was significantly more robust to clutter introduced during testing. As a generative model, RCN outperformed neural network models when tested on noisy and cluttered examples and generated realistic samples from one-shot training of handwritten characters. RCN also proved to be effective at an occlusion reasoning task that required identifying the precise relationships between characters at multiple points of overlap. On a standard benchmark for parsing text in natural scenes, RCN outperformed state-of-the-art deep-learning methods while requiring 300-fold less training data.CONCLUSIONOur work demonstrates that structured probabilistic models that incorporate inductive biases from neuroscience can lead to robust, generalizable machine learning models that learn with high data efficiency. In addition, our model{\textquoteright}s effectiveness in breaking text-based CAPTCHAs with very little training data suggests that websites should seek more robust mechanisms for detecting automated interactions.⇓Breaking CAPTCHAs using a generative vision model.Text-based CAPTCHAs exploit the data efficiency and generative aspects of human vision to create a challenging task for machines. By handling recognition and segmentation in a unified way, our model fundamentally breaks the defense of text-based CAPTCHAs. Shown are the parses by our model for a variety of CAPTCHAs .Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing{\textendash}based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects such as data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/358/6368/eaag2612},
	eprint = {http://science.sciencemag.org/content/358/6368/eaag2612.full.pdf},
	journal = {Science}
}

% Archetypes
@book{jung2014archetypes,
  title={The archetypes and the collective unconscious},
  author={Jung, Carl Gustav},
  year={2014},
  publisher={Routledge}
}

% Freud zitat
@book{samuels2003jung,
  title={Jung and the post-Jungians},
  author={Samuels, Andrew},
  year={2003},
  publisher={Routledge}
}
